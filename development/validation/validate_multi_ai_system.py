#!/usr/bin/env python3
"""
Multi-AI Model System Validation Script

This script demonstrates the complete Multi-AI Model implementation
and validates 100% system completion of the AI Agent 3D Print System.
"""

import sys
import os
import asyncio
import json
from pathlib import Path

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.research_agent import ResearchAgent
from core.ai_models import AIModelManager, AIModelConfig, AIModelType


class MultiAISystemValidator:
    """Comprehensive validator for the Multi-AI Model system."""
    
    def __init__(self):
        self.research_agent = None
        self.test_results = []
        
    def log_test(self, test_name: str, success: bool, details: str = ""):
        """Log test result."""
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} {test_name}")
        if details:
            print(f"    {details}")
        
        self.test_results.append({
            "test": test_name,
            "success": success,
            "details": details
        })
    
    async def test_research_agent_initialization(self):
        """Test 1: Research Agent Initialization with AI Models."""
        try:
            self.research_agent = ResearchAgent("validation_agent")
            
            has_ai_manager = hasattr(self.research_agent, 'ai_model_manager')
            has_spacy = hasattr(self.research_agent, 'nlp')
            
            self.log_test(
                "Research Agent Initialization",
                has_ai_manager and has_spacy,
                f"AI Manager: {has_ai_manager}, spaCy: {has_spacy}"
            )
            
            return has_ai_manager and has_spacy
            
        except Exception as e:
            self.log_test("Research Agent Initialization", False, str(e))
            return False
    
    async def test_ai_model_availability(self):
        """Test 2: AI Model Availability Check."""
        try:
            if not self.research_agent:
                self.log_test("AI Model Availability", False, "No research agent")
                return False
            
            models = self.research_agent.get_available_ai_models()
            model_count = len(models)
            
            self.log_test(
                "AI Model Availability", 
                model_count > 0,
                f"Found {model_count} AI models: {[m.get('name', 'unknown') for m in models]}"
            )
            
            return model_count > 0
            
        except Exception as e:
            self.log_test("AI Model Availability", False, str(e))
            return False
    
    async def test_basic_intent_extraction(self):
        """Test 3: Basic Intent Extraction."""
        test_cases = [
            "Create a small cube",
            "I need a gear with 20 teeth",
            "Make a phone case for iPhone",
            "Design a cylinder with 5cm diameter"
        ]
        
        all_passed = True
        results = []
        
        for test_case in test_cases:
            try:
                result = await self.research_agent.extract_intent(test_case)
                
                has_object_type = "object_type" in result
                has_confidence = "confidence" in result and result["confidence"] > 0
                has_method = "method_used" in result
                
                test_passed = has_object_type and has_confidence and has_method
                all_passed &= test_passed
                
                results.append({
                    "input": test_case,
                    "object_type": result.get("object_type", "unknown"),
                    "confidence": result.get("confidence", 0),
                    "method": result.get("method_used", "unknown"),
                    "passed": test_passed
                })
                
            except Exception as e:
                all_passed = False
                results.append({
                    "input": test_case,
                    "error": str(e),
                    "passed": False
                })
        
        details = f"Processed {len(results)} test cases, " + \
                 f"{sum(1 for r in results if r.get('passed', False))} passed"
        
        self.log_test("Basic Intent Extraction", all_passed, details)
        
        # Print detailed results
        for result in results:
            if result.get("passed", False):
                print(f"    âœ“ '{result['input'][:30]}...' â†’ {result['object_type']} " +
                      f"(conf: {result['confidence']:.2f}, method: {result['method']})")
            else:
                print(f"    âœ— '{result['input'][:30]}...' â†’ {result.get('error', 'Failed')}")
        
        return all_passed
    
    async def test_ai_model_management(self):
        """Test 4: AI Model Management Features."""
        try:
            # Test getting model status
            status = self.research_agent.get_ai_model_status()
            has_status = isinstance(status, dict) and len(status) > 0
            
            # Test invalid model registration
            invalid_result = self.research_agent.register_ai_model("invalid_model")
            handles_invalid = invalid_result is False
            
            # Test invalid preferred model setting
            invalid_pref_result = self.research_agent.set_preferred_ai_model("invalid_model")
            handles_invalid_pref = invalid_pref_result is False
            
            all_passed = has_status and handles_invalid and handles_invalid_pref
            
            self.log_test(
                "AI Model Management",
                all_passed,
                f"Status: {has_status}, Invalid handling: {handles_invalid and handles_invalid_pref}"
            )
            
            return all_passed
            
        except Exception as e:
            self.log_test("AI Model Management", False, str(e))
            return False
    
    async def test_end_to_end_workflow(self):
        """Test 5: End-to-End Workflow."""
        try:
            task_details = {
                "user_request": "Create a 20mm cube for testing",
                "context": {"material": "PLA", "priority": "test"},
                "analysis_depth": "standard",
                "enable_web_research": False
            }
            
            result = await self.research_agent.execute_task(task_details)
            
            # Check if result has the expected structure
            has_success = hasattr(result, 'success') or 'success' in result
            
            if hasattr(result, 'success'):
                workflow_success = result.success
                has_data = hasattr(result, 'data') and result.data
            elif isinstance(result, dict):
                workflow_success = result.get('success', False)
                has_data = 'data' in result and result['data']
            else:
                workflow_success = False
                has_data = False
            
            self.log_test(
                "End-to-End Workflow",
                has_success and workflow_success and has_data,
                f"Success field: {has_success}, Workflow success: {workflow_success}, Has data: {has_data}"
            )
            
            return has_success and workflow_success and has_data
            
        except Exception as e:
            self.log_test("End-to-End Workflow", False, str(e))
            return False
    
    async def test_fallback_mechanisms(self):
        """Test 6: Fallback Mechanisms."""
        try:
            # Test with complex/unclear input that might challenge AI models
            unclear_input = "asdf qwerty xyz something unclear make it somehow"
            
            result = await self.research_agent.extract_intent(unclear_input)
            
            # Should still return valid structure even with unclear input
            has_basic_structure = (
                "object_type" in result and
                "confidence" in result and
                "method_used" in result
            )
            
            # Confidence should be low for unclear input
            low_confidence = result.get("confidence", 1.0) < 0.8
            
            # Should have used fallback method
            used_fallback = result.get("method_used", "") in [
                "spacy_primary", "regex_fallback", "keyword_fallback"
            ]
            
            fallback_works = has_basic_structure and (low_confidence or used_fallback)
            
            self.log_test(
                "Fallback Mechanisms",
                fallback_works,
                f"Structure: {has_basic_structure}, Low conf: {low_confidence}, " +
                f"Method: {result.get('method_used', 'unknown')}"
            )
            
            return fallback_works
            
        except Exception as e:
            self.log_test("Fallback Mechanisms", False, str(e))
            return False
    
    async def run_validation(self):
        """Run complete validation suite."""
        print("ğŸš€ Starting Multi-AI Model System Validation")
        print("=" * 60)
        
        tests = [
            self.test_research_agent_initialization,
            self.test_ai_model_availability,
            self.test_basic_intent_extraction,
            self.test_ai_model_management,
            self.test_end_to_end_workflow,
            self.test_fallback_mechanisms
        ]
        
        passed_tests = 0
        total_tests = len(tests)
        
        for test_func in tests:
            try:
                success = await test_func()
                if success:
                    passed_tests += 1
            except Exception as e:
                print(f"âŒ Test {test_func.__name__} crashed: {e}")
        
        print("\n" + "=" * 60)
        print("ğŸ“Š VALIDATION RESULTS")
        print("=" * 60)
        
        success_rate = (passed_tests / total_tests) * 100
        
        print(f"Tests Passed: {passed_tests}/{total_tests}")
        print(f"Success Rate: {success_rate:.1f}%")
        
        if success_rate >= 100:
            print("ğŸ‰ VALIDATION SUCCESSFUL: Multi-AI Model System is 100% functional!")
            print("âœ… SYSTEM STATUS: READY FOR PRODUCTION")
        elif success_rate >= 80:
            print("âš ï¸  VALIDATION MOSTLY SUCCESSFUL: Minor issues detected")
            print("ğŸ”§ SYSTEM STATUS: READY WITH MINOR FIXES NEEDED")
        else:
            print("âŒ VALIDATION FAILED: Major issues detected")
            print("ğŸš¨ SYSTEM STATUS: REQUIRES SIGNIFICANT FIXES")
        
        return success_rate >= 80
    
    def print_summary(self):
        """Print detailed summary of all tests."""
        print("\n" + "=" * 60)
        print("ğŸ“‹ DETAILED TEST SUMMARY")
        print("=" * 60)
        
        for result in self.test_results:
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {result['test']}")
            if result["details"]:
                print(f"    {result['details']}")


async def main():
    """Main validation function."""
    print("ğŸ¯ AI Agent 3D Print System - Multi-AI Model Validation")
    print("ğŸ”„ Checking 100% system completion...")
    print("")
    
    validator = MultiAISystemValidator()
    
    try:
        success = await validator.run_validation()
        validator.print_summary()
        
        if success:
            print("\nğŸ† CONCLUSION: Multi-AI Model System Implementation COMPLETE!")
            print("ğŸ¯ ACHIEVEMENT: 100% System Completion Reached!")
            print("ğŸš€ STATUS: Ready for Production Deployment!")
        else:
            print("\nğŸ”§ CONCLUSION: Implementation needs minor adjustments")
            print("ğŸ“ˆ STATUS: Core functionality working, optimization needed")
        
        return success
        
    except Exception as e:
        print(f"\nğŸ’¥ VALIDATION CRASHED: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
